# -*- coding: utf-8 -*-
"""Copy of In_class_exercise_02.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PjYLWbsHLm9k9_5dkirJk5BQVV5csyxW

# **The second In-class-exercise**

(1) Write a Python program to find the duplicate elements in a given array of integers. Return -1 If there are no such elements.
"""

def duplicate(n):
  dup_ele = set() 
  dictn = {}

  for i in range(len(n)):
    if n[i] not in dictn.keys():
      dictn[n[i]] = 1
    else:
      dictn[n[i]] += 1
  for k in dictn.keys():
     if dictn[k] > 1:
        dup_ele.add(k)
  if dup_ele:
    return dup_ele
  else:
    return -1
 
n = [1,1,2,2,3,4,5,6,6,7,8,9,9]
m = [1,2,3,4,5]

print(duplicate(n))
print(duplicate(m))

"""(2) Write a Python program to select all the Sundays of a specified year."""

from datetime import date, timedelta
def all_sundays(year):
  d = date(year,1,1)
  d += timedelta(days = 6 - d.weekday())
  while d.year == year:
    yield d
    d += timedelta(days = 7)

for s in all_sundays(2019):
  print(s)

"""(3) Python files reading and writing. Download the “[exercise_02_data _collection.zip](https://github.com/unt-iialab/INFO5731_Spring2020/blob/master/In_class_exercise/exercise_02_data_collection.zip)” to your local and un-zip it.

*   Write a program to read all the txt files and save the sentences in all the files into one csv file with two columns, the first column is sentence id (txt file name+sentence line number), the second column is the sentence text content.
*   Remove all the punctuations from the sentences, save the processed sentences into a new column in the same csv file.
*   Ask the user to enter a word, return all the sentences that include this word, three kinds of information should be returned: sentence id, sentence text content, the count that user input word appear in the sentence..
"""

import os
from google.colab import files
load = files.upload()

import pandas as pd;
import csv as csv

column_id=[]
output_list=[]
new_list=[]
given_sentences=[]
for file in load:
  with open(file,'r') as f:
    given_text = f.read()
    given_sentences=given_text.splitlines()
    length_of_sentences=len(given_sentences)
    for a in range(length_of_sentence):
      column_id.append(file+'.'+str(a))
    for b in given_sentences:
      output_list.append(b)
with open('output_file.csv','w') as f:
  writer = csv.writer(f)
  for value in output_list:
    writer.writerow([value])
d1=pd.read_csv("output_file.csv",names=['text_with_punctuation'])
d2=pd.DataFrame(column_id,columns=['Sentence_ID'])
result=pd.concat([d1,d2],axis=1)
result=result.reindex(columns=['Sentence_ID','text_with_punctuation'])
result.to_csv('Final_file.csv')

"""(4) Install packages nltk, numpy, scipy, pandas, and sklearn on Google Colab. Write a program to test whether they are installed successfully."""

import numpy as np
a= np.array([[1,2,3],[2,3,4]])
print(a)
print("\n")

import pandas as pd
data = [1,2,3,4,5]
df = pd.DataFrame(data)
print(df)
print("\n")

import scipy
from scipy.constants import pi
print("sciPy - pi = %.16f"%scipy.constants.pi)
print("\n")

import sklearn
from sklearn import datasets
digits = datasets.load_digits()
print(digits)

import nltk
nltk.download()

nltk.sent_tokenize("Hello! I am Akshatha. How are you?")

nltk.word_tokenize("Hello! I am Akshatha. How are you?")